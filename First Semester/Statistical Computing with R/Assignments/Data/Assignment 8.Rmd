---
title: "Assignment 9"
author: "Durga Pokharel"
date: "04/01/2022"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

# [Q.N.1]Check the data with head(mtcars) and save a new data as mtcars.subset after dropping two non-numeric (binary) variables for PCA analysis 

```{r}
data <- mtcars
head(data)
str(data)
```
In our data vs and am are binary variable so I drop them here.
```{r message=FALSE}
library(dplyr)
mtcars.subset <- data[,-c(8,9)]
```

# [Q.N.2]  Fit PCA in the as mtcars.pca matcars.subset data with cor = TRUE and scores = TRUE) 

```{r message=FALSE}
mtcars.pca<-prcomp(mtcars.subset, cor = TRUE, scores = TRUE)
```
# [Q.N.3] Get summary of mtcars.pca and interpret standard deviation, proportion of variance carefully 

```{r}
summary(mtcars.pca)
```
From above summary we see when standard deviation is greater propertion of variance is high similarly when standard deviation is low propertion of variation is also low.

# [Q.N.4] Get eigenvalue of the components using standard deviation of mtcars.pca and chose the number of components based on Kaiser’s criteria 

```{r}
mtcars.pca$sdev ^2
```

# [Q.N.5]Get scree plot and chose the number of components best on “first bend” of this plot 

```{r}
#Calculating total variance explained by each principal component
var_explained<-mtcars.pca$sdev^2/sum(mtcars.pca$sdev^2)
##Creating scree plot 
library(ggplot2)
qplot(c(1:9), var_explained) + geom_line() + xlab("Principal Component") + ylab("Variance explained") + ggtitle("Scree Plot")
```
# [Q.N.6] Write how many components must be retained based on Kaiser’s rule and/or scree plot 

`solution`: Kisher’s rule suggaest us to use 2 components and Scree plot suggest us to retain 4 components for the problem.


# [Q.N.7] Fit the final PCA model based on the retained components and interpret it carefully 

```{r, message=FALSE}
library(psych)
mtcars.pca<- psych::principal(mtcars.subset, nfactors = 4, rotate = "none")
mtcars.pca

```
# [Q.N.8]Get the head of the saved loadings of mtcars.pca and interpret the values carefully 

```{r}
head(mtcars.pca)
```
# [Q.N.9] Retain two components, get their loadings and interpret them carefully 

```{r}
mtcars.pca_2 <- principal(mtcars.subset, nfactors = 2, rotate="none")
mtcars.pca_2$loadings
```
Principal components (PCs) are constructed by the linear combination of the original variables, where PCA loading are the coefficients. Here, cyl has the weights of 0.957 on PC1 computation but not in PC2.
Positive loading in above data indicates a variable and a component are positively correlated. Negative loading indicate a negative correlation between the variable and component. Similarly disp has positive loading with PC1 and negative loading with PC2.
Large (either positive or negative) loading indicate that a variable has a strong effect on that principal component. The larger value of cyl indicates the strong effect on PC1.

# [Q.N.10] Get biplot of these two component loadings and interpret it carefully 

```{r}
biplot(mtcars.pca, col = c("blue", "black"), cex = c(0.5, 1.3))
```

# [Q.N.11]Get the head of the saved scores of mtcars.pca and interpret carefully 

```{r}
head(mtcars.pca$scores)
```
The original ddataset is projected into four principal components.

# [Q.N.12] Get the head of the scores of first two components of mtcars.pca and intepret it carefully 

```{r}
head(mtcars.pca_2$scores)
```

# [Q.N.13] Get biplot of these two component scores and interpret it carefully 

```{r}
biplot(mtcars.pca_2, col = c("blue", "red"))
```
Here we can observe that hp, cyl, disp and wt contribute to PC1 with higher values. And mpg which has negative loadings is in opposite direction to PC1 with higher values. Gear and carb has higher contribution to PC2 with positive values and qsec has negative value.

# [Q.N.14] Get dissimilar distance of all the variables of mtcars data as mtcars.dist 

```{r}
#Distance calculation
mtcars.dist<- dist(mtcars.subset)
```

# [Q.N.15]Fit classical multi-dimensional scaling model with the mtcars.dist in 2-dimensional state as cars.mds.2d 

```{r}
# Fitting classical two- dimensional scaling model
cars.mds.2d<-cmdscale(mtcars.dist)
summary(cars.mds.2d)
```
# [Q.N.16] Plot the cars.mds.2d and compare it with the biplot of mtcars.pca and interpret it carefully.
```{r}
plot(cars.mds.2d, pch = 19)
abline(h = 0, v = 0, lty =2)
mtcars.subset<-mtcars[, 1:2] %>% scale
text(cars.mds.2d, pos = 4, labels = rownames(mtcars.subset), col = "tomato")
```
Hornet 4 Drive, Pontiac Fire bird etc lies on the positive orthant which means they have positive contribution to the first and second components. However, Lotus Europa and Ferari has opposite but highest weight component 1 and component 2.

# [Q.N.17] Fit classical multi-dimensional scaling model with the mtcars.dist in 3-dimensional state as cars.mds.3d 
```{r}
#Fiting multi-dimensional scaling model with mtcars.dist in 3 - dimensional state
cars.mds.3d<-cmdscale(mtcars.dist, k = 3)
summary(cars.mds.3d)
```
# [Q.N.18] Create a 3-d scatterplot of cars.mds.3d with type = “h”, pch=20 and lty.hplot=2 and interpret it carefully 

```{r}
library(scatterplot3d)
cars.mds.3d <- data.frame(cmdscale(mtcars.dist, k = 3))
scatterplot3d(cars.mds.3d, type = "h", pch = 19, lty.hplot = 2)
```
# [Q.N.19] Create a 3-d scatterplot of cars.mds.3d with type = “h”, pch=20, lty.hplot=2 and color=mtcars$cyl and interpret it carefully 
```{r}
library(scatterplot3d)
cars.mds.3d <- data.frame(cmdscale(mtcars.dist, k = 3))
scatterplot3d(cars.mds.3d, type = "h", pch = 19, lty.hplot = 2, color = mtcars$cyl)
```
We plotted the principal components in 3- dimensional scatter plot which is distinguished by color cyl. We can use higher dimensions by changing the k argument in the cmdscale() function to a higher value for eg. k = 3 for 3 dimensuon.

# [Q.N.20] Write a summary comparing PCA and MDS fits done above for mtcars data  

`Solution`: The input to PCA is the original vectors in n-dimensional space. Similarly, input to MDS is the pairwise distances between points. PCA behaves as an algorithm but MDS is a visualization technique for any factor analysis. MDS applies PCA for the dimensionality reduction.
For the mtcars, it shows that two or more but less than or equal to 5 latent features can be generated from the given dataset.





